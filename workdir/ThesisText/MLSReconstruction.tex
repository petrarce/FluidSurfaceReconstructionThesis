\chapter{Moving least squares level set smoothing} \label{sec:mls}
\section{Introduction and motivation}
As were previously concluded blur level set method requires a high effort to configure all available parameters to match the required fluid surface reconstruction quality.\\
A lot of work was already done in the field of Surface reconstruction from point clouds. 
The central objective is obtaining a digital representation of real-world objects, where the basic problem is to capture a 3D point cloud that samples the real world and reconstruct as much information as possible concerning the scanned objects. There exists a large number of approaches, that reconstruct surfaces from the point cloud. different algorithms make an assumption regarding the quality of the point cloud (density, noise, outliers...) on which the algorithm is supposed to perform robustly.\\
Points that are randomly distributed near the surface are traditionally considered to be noise. The specific distribution is commonly a function of scanning artifacts such as sensor noise, depth quantization, and distance or orientation of the surface within the scanner. For some popular scanners, noise is introduced along the line of sight and can be impacted by surface properties, including scattering characteristics of materials. In the presence of such noise, the typical goal of surface reconstruction algorithms is to produce a surface that passes near the points without overfitting to the noise. Robust algorithms that impose smoothness on the output \cite{PSR}, as well as methods that employ robust statistics \cite{FPPSKR}, are common ways of handling noise. We note that spatially varying noise poses a significant challenge \cite{NSRFRPS}, where for many scanners, the noise level is correlated with the depth measurement.\\
In the work, \cite{Apss} new Point Set Surface (PSS) definition based on moving least squares (MLS) fitting of algebraic spheres is presented. The central advantages of the APSS approach compared to existing planar MLS include significantly improved stability of the projection under low sampling rates and in the presence of high curvature.\\
In this thesis ideas developed in \cite{Apss} will be applied directly to the SDF of the MC grid domain. As a blur level set method, the level set MLS correction method is also designed to be applied on every reconstruction approach, which in the core uses an implicit SDF ISO-surfaces for surface reconstruction. The MLS method is capable of recunstruction large and complex surfaces with a millions of particles preserving small features sharp features smoothing out a flat surface areas (see figure \ref{fig:mls_motivation}).
\begin{figure}
	\begin{center}
		% \begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionOriginal1.png}
		% \end{subfigure}
	\end{center}
	\caption{original surface}
	\label{fig:mls_motivation}
\end{figure}
\begin{figure}
	\begin{center}
		% \begin{subfigure}[b]{\textwidth}
		\includegraphics[width=\textwidth]{figures/CanionMls1.png}
		% \end{subfigure}
	\end{center}
	\caption{Mls smoothed surface}
	\label{fig:mls_motivation}
\end{figure}
\section{Mls surface computation}

The core of the smoothing method is a computation of an approximated surface. 
The main idea of the computational approach was taken from the work of Guennebaud \& Gross \cite{Apss}. 
Given a set of points $P = \{p_i \in R_d \}$, a smooth surface $S_P$ approximating $P$ using a moving least squares spherical fit to the data can be defined. However, in the case of this study, there is no set of points, that define the surface and which should be approximated. As an input a set of points defined in the MC domain is given. 
Although, with the MC grid vertices we also have values of the SDF, which theoretically define the distance to the surface. But in most cases, the computed SDF doesn't represent an actual distance to the surface, but just an approximation of the distance, which is more approximate in the neighborhood of the 0-level iso-surface, but does not define the distance to the surface in the more distant areas from the fluid surface. 
However, in the 0-level intersection areas, SDF also doesn't exactly define a distance to the surface, which is one of the reasons for the occurrence of small frequency bumps on the reconstructed fluid surface. Thus applying MLS approximation to the 0-level intersection MC vertices and correcting their SDF is an attempt to reconstruct a smooth distance-based SDF.\\
In the APSS work analytical equation of the sphere was used as a core function, which is approximated by applying least-squares minimization to a sample set. Given a point $x$ the equation, that represents a distance to the sphere center can be defined as a function of x:
\begin{equation}
f(x) = s_1\cdot x^2 + s_2 \cdot x_x + s_3 \cdot x_y + s_4 \cdot x_z + s_5
\end{equation}
where $x^2$ is a dot product of point $x$ on itself, $s_i$ are unknown coefficients, that are going to be calculated. Thus the goal is to calculate the unknown coefficients $s_i$, and based on the approximated MLS surface correct the SDF value by multiplying the MC grid vertex world coordinate by the coefficients:
\begin{equation}
SDF_{new}(x_i) = f(x_i) = s_1\cdot x_i^2 + s_2 \cdot x_{i_x} + s_3 \cdot x_{i_y} + s_4 \cdot x_{i_z} + s_5 \label{eq:sdf_approximation}
\end{equation}
In the Algorithm \ref{alg:mls_alg} each MC grid vertex in the cluster $SDF_{new}$ is computed. As soon as each vertex can appear in multiple clusters the weighted sum of the SDF corrections is computed. In case of this work it was decided to use a uniform weights for each SDF corrected value to maximize smoothing. Thus the final SDF value can be interpreted as an average among all computed SDF approximations:
\begin{equation}
	SDF_{mls}(x) = \dfrac{\sum_{j \in Clusters}{SDF_{j}(x)}}{|Clusters|} \label{eq:sdf_mls_approximation_final}
\end{equation}
where $Clusters$ is a set of clusters, such that $: \forall cluster \in Clusters: x \in cluster$. 

\section{Algorithm}
The general idea of the algorithm is to extract a subset of the MC grid vertices, which are representatives of a 0-level intersection of the signed scalar field (vertices which are used for  final surface mesh generation by linearly interpolating of the SDF values between the neighboring vertices with different signs). For each of the extracted vertices find appropriate neighborhood that consists of 0-level intersection vertices. This neighborhood is a representative of a local area of fluid surface. Further term cluster will be used to refer to the local 0-level intersection neighborhood of a MC grid vertex.\\
Given a generated cluster MLS surface approximation is computed and for each vertex in the cluster SDF approximation is computed according to equation \ref{eq:sdf_approximation}. All SDF approximations computed within different clusters for one vertex is summed up and average of the sum is a final SDF approximation for the observer vertex according equation \ref{eq:sdf_mls_approximation_final}.\\
In the Algorithm \ref{alg:mls_alg} general overview of the MLS smoothing filter on the MC SDF grid is described. The filter can be applied iteratively, similarly to the blur filter.
\begin{algorithm}[H]
	\scriptsize
	\begin{algorithmic}
		\State $ZeroLevelIntersectionSet \gets computeZeroLevelIntersectionSet(oldSDF)$
		\ForAll{$vertex \in ZeroLevelIntersectionSet$}
			\State $neighborClusters \gets neighborClusters \cup computeCluster(vertex)$
		\EndFor
			
		\ForAll{$cluster \in neighborClusters$}
			\State compute $surfApproximation$ MLS surface approximation for $cluster$
			\ForAll{$vertex \in cluster$}
				\State $newSDF[vertex] \gets newSDF[vertex] + computeUpdatedSDF(vertex, surfApproximation)$
				\State $weights[vertex] \gets weights[vertex] + 1$
			\EndFor
		\EndFor

		\ForAll{$\{vertex, weight\} \in weights$}
			\State $sdfFactor \gets min\left(1, smoothingFactor \cdot \dfrac{fluidParticles[vertex]}{maxFluidParticles}\right)^2$
			\State $newSDF[vertex] \gets sdfFactor \cdot \dfrac{newSDF[vertex]}{weight} + oldSDF[vertex] \cdot (1 - sdfFactor)$
		\EndFor
		\State return newSDF
	\end{algorithmic}
	\caption{mls smoothing filter algorithm}
	\label{alg:mls_alg}
\end{algorithm}
where $oldSDF$ is a computed signed scalar field received from the underlying reconstruction method (ZhuBridson, Solenthaler, etc.), $ZeroLevelIntersectionSet$ is a set of MC grid vertices that represents a 0-level intersection area of the signed scalar field, $neighborClusters$ is a set of clusters computed for each vertex in $ZeroLevelIntersectionSet$, $surfApproximation$ is a set of unknown coefficients of the surface approximation sphere computed using WLLS method, $newSDF$ is a signed scalar field where approximated SDF values are stored, $weights$ is a buffer containing weights of each MLS value (in our case it can be interpreted as a counter of the number of times vertex appear in different clusters), $smoothingFactor$ is a coefficient which defines how much of the MLS approximation should be applied (the concept is described in details in section \ref{sec:smoothing_factor}), $fluidParticles$ is a set of counters, which contains the number of fluid particles within the support radius of MS grid vertex, $maxFluidParticles$ is a potential maximum of the fluid particles in the neighborhood of the MC grid vertex computed as $maxFluidParticles = \dfrac{supportAreaVolume}{fluidParticleVolume}$.\\
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=\textwidth]{figures/MlsIntersectionVertexSet.png}
	\end{center}
	\caption{0-level intersection MC grid vertices. Red are vertices with positive SDF sign, blue are one with negative sign}
	\label{fig:intersection_vertices}
\end{figure}
As an input we have an old SDF, computed by underlying reconstruction method (e.g. Mullert et.al., Zhu \& Bridson, Solenthaler etc.). The algorithm computes 0-level intersection vertices. This is required as the MLS by definition reconstructs surfaces from SDF that represents a distance to a surface (e.g. tuple $(x_p, u_p)$ is a position in space and distance to the surface). However, in the underlying reconstruction methods computed SDF does not represent the distance to a surface along the whole domain of MC grid. Only the SDF values of 0-level intersection vertices are assumed to be distances and, thus can be used for MLS approximation. Intersection cells are shown in the Figure \ref{fig:intersection_vertices}.\\

Another advantage of using set of 0-level intersection vertices for MLS approximation is that the algorithm can exactly determine the set of neighbor MC vertices that lie along the reconstructed surface, without approximating the surface normal and taking approximate neighbor set along the tangential direction to the normal. Some computed clusters are represented in the Figure \ref{fig:clusters}.\\

And the last important advantage is a performance optimization, as there is no need to compute costly MLS approximation for each MC grid vertices in the domain.\\

However, there is no guarantee that after the applying approximation the SDF sign of the MC grid vertex will not be switched, e.g. if some MC grid vertex had a positive SDF value before smoothing, after MLS approximation it can take a negative value. In this case, the set of 0-level intersection vertices set will be changed, and some of the MC grid vertices, that were not MLS smoothed will be used during the final surface generation and can bring unnecessary noise to the reconstructed surface. This issue can be worked around by applying multiple iterations of MLS smoothing trading off final surface smoothness and reconstruction performance.\\
\begin{figure}[H]
	\begin{center}
		\begin{subfigure}[b]{0.45\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsCluster.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsCluster2.png}
		\end{subfigure}
	\end{center}
	\caption{Examples of computed clusters. Red sphere is a reference vertex w.r.t. which cluster is computed.}
	\label{fig:clusters}
\end{figure}

The set of 0-level intersection vertices is split into clusters. For each cluster coefficients of MLS approximated surface is computed using the WLLS approach, and based on the obtained solution new SDF values are generated for each vertex in the cluster. As soon as vertices can appear in multiple clusters the total values are summed up and average is computed according to equation \ref{eq:sdf_mls_approximation_final}.\\

For thin and small feature areas, where smoothing could destroy surface details or even introduce artifacts, the smoothing factor is computed and applied to the final MLS value as a weighted sum between the old SDF value and new MLS corrected SDF value. The smoothing factor is applied similarly to Blur filter (see section \ref{sec:smoothing_factor} for more information on detection thin areas and computation of smoothing factor).\\

\subsection{Clusters computation}
For each vertex in  0-level intersection vertex-set, clusters are computed. The clusters are a set of neighbor vertices to currently observer vertex, picked from the 0-level intersection vertices set, which will be used to compute an MLS approximation. The Algorithm \ref{alg:computeClusters} describes the procedure of clusters generation.
\begin{algorithm}[H]
	\scriptsize
	\begin{algorithmic}
			\State $vertices \gets sampleVertices(ZeroLevelIntersectionSet)$
			\ForAll{$vtx \in vertices$}
				\State compute $curvature_{vtx}$

				\State $maxSamplesFactor \gets \left(\dfrac{min(\dfrac{1}{|curvature_{vtx}|}, FluidParticleDiameter \cdot FluidParticlesInCurvatureRadius)}{FluidParticleDiameter \cdot FluidParticlesInCurvatureRadius}\right)^2$
				\State $samplesCount \gets MlsSamples \cdot maxSamplesFactor$

				\State $cluster \gets getNeighbourCells(ZeroLevelIntersectionSet, vtx, samplesCount)$
				\State $clusters \gets clusters \cup cluster$ 
			\EndFor
			\State return clusters
	\end{algorithmic}
	\caption{mls clusters computation}
	\label{alg:computeClusters}
\end{algorithm}
where $ZeroLevelIntersectionSet$ is a previously computed set of MC grid vertices that are in the neighborhood of the 0-level of the signed scalar field, $FluidParticleDiameter$ is a diameter of a SPH fluid particles, $FluidParticlesInCurvatureRadius$ is a number of particles that can fit in a mean curvature radius (more explanations later), $MlsSamples$ - user defined maximum number of neighbor vertices in the cluster.\\
Important role in the MLS approximation takes number of samples in the cluster. The more samples will be used the more smoothing will be applied to the vertices. Thus as a general rule of thumb the more curvy and detail surface in the area of interest the less samples should be used for MLS approximation. This also gives a performance optimization during computation of unknown coefficients of the approximation sphere.\\
Before the algorithm iterates over the vertices in the 0-level intersection set, they are sampled in separate storage $vertices$ (for the motivation and more details see section \ref{sec:clusterDencityFactor}). In the mean time unchanged set of $ZeroLevelIntersectionSet$ is to be used in the neighborhood search algorithm for clusters. This step adds a trade-off between the reconstruction runtime and surface quality.\\
Next the curvature of the current processed vertex is computed. To save sharp features of the fluid surface it was decided to decrease the number of samples in clusters with high curvature of the central vertex in the area. Based on the mean curvature of current 0-level intersection vertex number of samples in the cluster is computed, which is a fraction [1, MlsSamples]. The less samples are in the smaller the difference between the original and approximated SDF, which is exactly what is intended for areas of the fluid with splashes or waves on the surface.\\
The curvature is computed using the method described in \cite{CurvatureComputation}. First of all, compute the curvature for each surface particle that resides in this cell with the help of its neighboring particles as:
\begin{equation}
	c_i = \sum_j{(1 - n_i \cdot n_j)\cdot k_c(x_i-x_j, h)}
\end{equation}
where $j$ stands for the neighboring particles, $n_i, n_j$ are SPH particle normals and
$k_c$ is the kernel function defined as:
\begin{equation}
	k_c(x_i-x_j, h) = max\left(0, \dfrac{1 - |x_i - x_j|}{h}\right)
\end{equation}
Then the curvature of any surface cell is approximated as:
\begin{equation}
	c_{cell} = \dfrac{\sum_i{c_i}}{N}
\end{equation}
where $i$ and $N$ represent the surface particles and the total number of surface particles inside the cell, respectively. The $maxSamplesFactor$ factor reaches valuation of 1 for all vertices, where mean curvature radius exceeds value of $FluidParticleDiameter \cdot FluidParticlesInCurvatureRadius$. Optimal solution in this work was $FluidParticlesInCurvatureRadius=50$. To understand better the idea of $FluidParticlesInCurvatureRadius$ see figure \ref{fig:FluidParticlesInCurvatureRadius}:
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{figures/ParticlesInCurvatureRatius.png}
	\end{center}
	\caption{Representation of idea of fluid particles in curvature radius. Blue line is a fluid surface, red circle is a circle which approximates the surface in of the fluid in the local neighborhood, $R_c$ is a radius of the sphere that is inversely proportional to a curvature in this area, $fd$ is a diameter of a fluid particle.} \label{fig:FluidParticlesInCurvatureRadius}
	\label{fig:FluidParticlesInCurvatureRadius}
\end{figure}
According to the figure \ref{fig:FluidParticlesInCurvatureRadius} FluidParticlesInCurvatureRadius is a number of fluid particles in a row that can be fit into the fluid radius. This metric is used while it is portable over different simulation scenarios that can have distinct diameters of fluid particle (in this case constant diameter of fluid particles along the whole simulation is assumed).

\subsection{MLS neighborhood search}
MLS approximation is always performed on a set of samples in a point cloud. In our case a set of samples, a cluster, is a set which is supplied to the SLAU solver to compute unknown coefficient of the required approximation function. The idea of the clusters generation algorithm is, given a 0-level intersection vertices set and a reference vertex of MC grid, to pick up specified number of samples from the 0-level intersection set, closest to the reference one. To do this, starting from the reference vertex potential nearest vertices of MC grid are queried, checked if they are in 0-level set and are not yet in the cluster set. An accepted vertices are deployed into a cluster set and further queries are performed within the newly deployed vertices until the required size a cluster is reached.
Neighbors detection is described in pseudo-code of Algorithm \ref{alg:mls_nbsearch}.
\begin{algorithm}[H]
	\scriptsize
	\begin{algorithmic}
		\State $todoMlsVertices \gets \{ BaseMcVertex \}$
		\While{$sizeof(neighborCells) < MlsSamples \land todoMlsVertices \ne \emptyset$}
			\State $currentMcVertex \gets todoMlsVertices[0]$
			\State $todoMlsVertices \gets todoMlsVertices \ todoMlsVertices[0]$
			\If{$indexOf(currentMcVertex) \in SelectedIndices \lor SelectedIndices = \emptyset$}
				\State $neighborCells \gets neighborCells \cup currentMcVertex$ 
			\EndIf
			\ForAll{ $nc \in \text{nearest neighbors of currentMcVertex}$}
				\If{$nc \in ZeroLevelIntersectionSet \land nc \not\in neighborCells \land nc \not\in todoMlsVertices$}
					\State $todoMlsVertices \gets todoMlsVertices \cup nc$
				\EndIf
			\EndFor
		\EndWhile
		\State return $neighborCells$;
	\end{algorithmic}
	\caption{MLS MC vertex neighbors search}
	\label{alg:mls_nbsearch}
\end{algorithm}
As an input procedure receives a $BaseMcVertex$ - a MC vertex for which neighborhood cluster is computed, $ZeroLevelIntersectionSet$ - 0-level intersection MC vertices explained previously, $SelectedIndices$ - a set of indexes of neighbors, that should be accepted as a samples (more explanations are in the next section). The nearest neighbors of $currentMcVertex$ can be seen on the figure \ref{fig:MCgridNearestNeighbors}.\\
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.1\textwidth]{figures/nearestNeighbors.png}
	\end{center}
	\caption{Nearest neighbors (blue) of the reference (red) MC grid vertex (2D example) } \label{fig:MCgridNearestNeighbors}
\end{figure}

The procedure returns a set of MC grid vertices, that are in the nearest neighborhood within a requested $BaseMcVertex$, and which are in a $ZeroLevelIntersectionSet$ set. The computed array has some important properties, that will be exploited further. First of all, the further the neighbor is from the base cell the higher index it has inside the array. Figure \ref{fig:mls_neighbor_alignment} shows the example of alignment of detected cluster neighbors in the set of 0-level intersection vertices.\\
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{figures/MlsSamplesBufferAlignment.png}
	\end{center}
	\caption{Mapping of MLS samples from cluster to buffer (2D slice view)} \label{fig:mls_neighbor_alignment}
\end{figure}
\subsubsection{MlsSamples}
The final neighborhood is formed as an area of MC vertices along the 0-level iso-surface. The larger number of MLS samples the wider area of a cluster and more smoothing will be applied to the SDF. Some examples of computed neighborhood area depending on the amount of MlsSamples. Corresponding reconstructed surface are shown in the figure \ref{fig:mls_samples_example_surfaces}.
\begin{figure}[H]
	\begin{center}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSamples10.png}
			\caption{10 samples}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSamples40.png}
			\caption{40 samples}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSamples80png.png}
			\caption{80 samples}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSamples100.png}
			\caption{100 samples}
		\end{subfigure}
	\end{center}
	\caption{Surface quality depending on the number of MLS samples} \label{fig:mls_samples_example_surfaces}
\end{figure}
Thus modifying the amount of MLS samples influences the final surface quality as well as reconstruction time. 
\subsection{Algorithm optimizations}
In general complexity of the MLS smoothing algorithm has a $O(n \cdot m)$, where $m$ is the number of MLS samples and $n$ is an number of 0-level intersection vertices. However, without sacrefizing the smoothing quality the subsets of $m$ and $n$ can be picked to perform MLS approximation. Next sections will describe a techniques that reduces number of samples and vertices for which MLS approximation is performed 
\subsubsection{SelectedIndices}

To receive a smooth surface it is required to use a large amount of MLS samples in the neighborhood as was already shown in the previous section. Thus it requires a lot of computational efforts, to generate approximated MLS surface and to correct SDF of 0-level vertices. Thus to reduce reconstruction time it was decided to apply the Monte Carlo approach to pick a representative subset of samples from the neighborhood cluster, which uniformly represents the initially computed set. The hope is that distribution will detect correctly so that the final surface quality will not be degraded too much and the computation time of the reconstruction phase will be reduced without introducing artifacts into the final fluid surface.\\
The first idea that comes to mind is just uniformly pick $n$ samples from the computed MLS sample buffer (see figure \ref{fig:mls_neighbor_alignment}). But for our needs it is more appropriate to pick samples uniformly from each distance level from the reference sample (central one). Figure \ref{fig:samples_distribution} shows the example of the uniformly distributed samples over distance levels.
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.48\textwidth}
			\includegraphics[width=\textwidth]{figures/SampleDistrUnifOverDomain.png}
			\caption{Samples uniformly picked w.r.t. the samples domain}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\textwidth}
			\includegraphics[width=\textwidth]{figures/SampleDistrOverLeveDist.png}
			\caption{Samples uniformly picked from w.r.t. the distance level}
		\end{subfigure}

	\end{center}
	\caption{Example of uniform sampling over points domain and over distance levels. Yellow circle is a reference sample, green are accepted samples (picked during sampling), empty are dropped samples.}
	\label{fig:samples_distribution}
\end{figure}

The probability of picking samples far away from the base sample is larger than the probability of picking the samples near the base cell, e.g. (for simplicity circle will be used to show the probability distribution) the density function of picking an arbitrary point, that lies in radius $r$ from the current point is shown in equation \ref{eq:probability_distr}.
\begin{equation}
	p(r) = \dfrac{2 \cdot \pi r}{\pi \cdot R^2} = \dfrac{2 \cdot r}{R^2}
	\label{eq:probability_distr}
\end{equation}
where $R$ is a radius to the sphere (distance to the outer distance level from reference point). In another words talking the probability of picking sample on that lies from the central sample in a distance of $r$ is equal to length of the inner circle with radius $r$ divided by the area of the sphere.
Thus to pick samples from the buffer uniformly w.r.t. the distance level from the central sample we have to modify the probability distribution of picking a samples from buffer. Next procedure is applied to pick the sample indexes from the buffer uniformly w.r.t. the distance level:
\begin{algorithm}[H]
	\scriptsize
	\begin{algorithmic}
			\State $acceptedIndices \gets \{0\}$;
			\State $lowerBound \gets 0$
			\State $upperBound \gets (MlsSamples - 1)$
			\For{$i \in [0, MaxSamples]$}
				\State $offset \gets \dfrac{PickRandom(0, MaxSamples - 1)}{MaxSamples}$
				\State $index \gets lowerBound + offset^2 \cdot (upperBound - lowerBound))$
				\State $currentIndex \gets index$
				\While{$index \in acceptedIndices$}
					\If{$index = lowerBound$}
						\State $index = upperBound$
					\Else
						\State $index \gets index - 1$
					\EndIf
				\EndWhile
				\State $acceptedIndices \gets acceptedIndices \cup index$;
				\If{$index = lowerBound$}
					\State $index++;$
					\While{$index \in acceptedIndices$}
						\State $index++$
					\EndWhile
					\State $lowerBound \gets index$
				\Else 
					\If{$index = upperBound$}
						\State $index--;$
						\While{$index \in acceptedIndices$}
							\State $index--$
						\EndWhile
						\State $upperBound \gets currentIndex$
					\EndIf
				\EndIf
			\EndFor
	\end{algorithmic}
	\caption{random sampling of indices's in the MLS neighborhood}
	\label{alg:mls_montecarlo_sampling}
\end{algorithm}
The idea of the algorithm is to generate set of indexes from domain of MLS neighborhood buffer, and after that shift the index to the beginning according to quadratic function $x^2$ where $x\in [0,1]$ is uniformly at random generated position in the array. This way the probability distribution of picking sample from specific distance from the center of the area will be compensated and converge to uniform distribution. Figure \ref{fig:distributions} shows the distributions of sampling indexes before quadratic shifting (uniformly pick the index of the buffer) and after (pick index uniformly and shift it according to the $x^2$ function).\\
\begin{figure}
	\begin{tikzpicture}
		\begin{axis}
			[
				xlabel={$x$},
				ylabel={$y$},
				ymin = 0,
				ymax = 0.1,
				width = 0.45\textwidth
			]
			\addplot[
				red,
				mark=*
			] table{initialDistributionOverLevels.txt};
		\end{axis}

	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}
			[
				xlabel={$x$},
				ylabel={$y$},
				ymin = 0,
				ymax = 0.1,
				width = 0.45\textwidth
			]
			\addplot[
				blue,
				mark=*
			] table{resultingDistributionOverLevels.txt};
		\end{axis}
	\end{tikzpicture}
	\caption{Initial distribution of the samples over area levels (left) and final distribution after modifying indexes (right)}
	\label{fig:distributions}
\end{figure}
where $x$ is a probability of picking sample on distance level $i$ from the center of area, $y$ - is a depth of the distance level. The distributions where generated empirically by sampling 10000 indices's over the whole domain of the buffer. 
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.9\textwidth}
			\includegraphics[width=\textwidth]{figures/SamplingFullDomain.png}
			\caption{full set of neighbors}
		\end{subfigure}
		\begin{subfigure}[b]{0.9\textwidth}
			\includegraphics[width=\textwidth]{figures/SamplingQuadratic.png}
			\caption{set sampled with quadratic shift}
		\end{subfigure}
		\begin{subfigure}[b]{0.9\textwidth}
			\includegraphics[width=\textwidth]{figures/SamplingUniform.png}
			\caption{uniformly sampled set}
		\end{subfigure}
	\end{center}
	\caption{sampling of MLS neighbor vertices set} \label{fig:cluster_sampled}
\end{figure}
In the quadratic shift sampling approach most of the samples are concentrated in the center of the cluster, thus compensating the number of samples, that are far away from the center and that are near to the center. In the uniform approach samples are spread along the whole area. Figure \ref{fig:cluster_sampled} shows the cluster, with different sampling types.\\
In Figure \ref{fig:surface_sampling_results} some examples of reconstructed scenes using sampled subset of neighbor vertices is shown.
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/MLSSurfaceOriginal.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/MLSSurfaceSamplingFullSet.png}
			\caption{mls corrected surface with full set of samples}
		\end{subfigure}
		\begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/MLSSurfaceSamplingQuarterSet.png}
			\caption{mls corrected surface with quarter set of samples}
		\end{subfigure}
	\end{center}
	\caption{Reconstructed MLS surface} \label{fig:surface_sampling_results}
\end{figure}
Both, full samples set and partial samples set removes bumps from the original surface, but for the reconstruction with $\dfrac{1}{4}$ set of MLS samples small frequency noise can be seen on the corner areas.\\

\subsubsection{Clusters density factor} \label{sec:clusterDencityFactor}
According to the algorithm \ref{alg:mls_alg} clusters are computed for each 0-level intersection cell, and after this MLS approximation is applied to each intersection cell in the cluster. This means that each 0-level intersection MC vertex takes part in multiple clusters during the MLS approximation. However, for fluid surface smoothing it is enough to generate clusters only for a subset of the 0-level intersection cells such that depending on the size of the cluster all 0-level intersection vertices will be MLS corrected. For example see figure \ref{fig:mls_clusters_sparse_generations}:
\begin{figure}[h]
	\begin{center}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/FullDomainClusters.png}
			\caption{} \label{fig:mls_clusters_sparse_generations_nonsparse}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/PartialDomainClusters.png}
			\caption{} \label{fig:mls_clusters_sparse_generations_sparse}
		\end{subfigure}
	\end{center}
	\caption{Comparison of sparse and non sparse clusters computation.} \label{fig:mls_clusters_sparse_generations}
\end{figure} 
Filled vertices are the MC grid vertices for which clusters are generated, and then MLS correction is computed. Generating clusters for each 0-level intersection cell might be redundant, thus clusters can be computed not for all domain of 0-level intersection vertices but just for a subset of the 0-level intersection vertices domain (see figure \ref{fig:mls_clusters_sparse_generations_nonsparse}). However, it is important to generate clusters such that they overlap, while for different clusters different surface approximations are computed, and thus in the border areas, non-smooth transitions can be observed. To accomplish this problem ClusterFactor ($cf$) is used. ClusterFactor identifies the fraction of the 0-level intersection vertices for which clusters will be computed. For simplicity, $cf \cdot n$ out of $n$ 0-level intersection vertices will be picked uniformly at random. Figure \ref{fig:mls_sparse_clusters_reconstruction} shows the results of the reconstruction given different valuations of $ClusterFactor$.
\begin{figure}[h]
	\begin{center}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSparseClustersOriginalpng.png}
			\caption{Original reconstruction} \label{fig:mls_clusters_original}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSparseClusters0.01.png}
			\caption{ClusterFactor = 0.01} \label{fig:mls_clusters_sparse_0.01}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSparseClusters0.5.png}
			\caption{ClusterFactor = 0.5} \label{fig:mls_clusters_sparse_0.5}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSparseClusters1.png}
			\caption{ClusterFactor = 1} \label{fig:mls_clusters_sparse_1}
		\end{subfigure}

	\end{center}
	\caption{Comparison reconstruction which uses full domain of 0-level intersection cells for clusters computation and subset of 0-level intersection cells depending on the ClusterFactor.} \label{fig:mls_sparse_clusters_reconstruction}
\end{figure} 
Taking into account that the number of MLS vertices in the cluster is 100, MLS smoothing over the 0-level intersection vertices with ClusterFactor 0.01 is generating clusters without overlap, thus on the figure \ref{fig:mls_clusters_sparse_0.01} the reconstructed surface looks such as if the surface was built from parts of spheres. In the other hand taking $ClusterFactor = 0.5$ reconstructed surface looks much smoother (see figure \ref{fig:mls_clusters_sparse_0.5}). In this thesis $ClusterFactor = 0.5$ was used for reconstruction of fluid surfaces. 


\subsection{Applying MLS filter iteratively}
In the Algorithm \ref{alg:mls_alg} it is stated that only computed 0-level intersection MC grid vertices are under the MLS correction procedure. Other vertices that are not in the neighborhood to the 0-level of the scalar signed fielddoes not take part in MLS smoothing. In this case, the next problem can arise: the corrected MC vertices can change the sign of SDF and will be no more in a subset of the 0-level intersection MC grid vertices, but other vertices that previously was in the 0-level intersection set will enter this set from after the smoothing. In the Figure \ref{fig:mls_iterations} shown the effect of applying MLS correction on the 0-level intersection MC grid vertices:
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.9\textwidth}
			\includegraphics[width=\textwidth]{figures/IntersectionCellsSdfBeforeMls_1_iter.png}	
			\caption{Before MLS smoothing} \label{fig:mls_0_iter}
		\end{subfigure}
		\begin{subfigure}[b]{0.9\textwidth}
			\includegraphics[width=\textwidth]{ figures/IntersectionCellsSdfAfterMls_1_iter.png}	
			\caption{1 iterations of MLS smoothing} \label{fig:mls_1_iter}
		\end{subfigure}
		\begin{subfigure}[b]{0.9\textwidth}
			\includegraphics[width=\textwidth]{figures/IntersectionCellsSdfAfterMls_3_iter.png}	
			\caption{3 iterations of MLS smoothing} \label{fig:mls_3_iter}
		\end{subfigure}
	\end{center}
	\caption{Subset of 0-level intersection MC grid vertices with SDF values before and after MLS smoothing. Red are vertices with positive SDF valuations, blue are vertices with negative SDF value.} 
	\label{fig:mls_iterations}
\end{figure}
After application of MLS it can be seen on the picture, that some MC grid vertices changed their SDF sign from negative to positive (Figure \ref{fig:mls_1_iter}). After 3 iterations of MLS filter smoothing the effect of the SDF sign switch is applied to a smaller subset of MC grid vertices (Figure \ref{fig:mls_3_iter}). This can be explained by the convergence of the surface to some invariant state since each iteration the average standard deviation of SDF values before and after the application of MLS among all clusters gradually decreases and converges to 0 (see figure \ref{fig:mls_std_dev}).
\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}
				[
					xlabel=iterations,
					ylabel=error,
					width = 0.45\textwidth
				]
				\addplot[
					red,
					mark=*
				] table{mls_std_dev.txt};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{Total standard deviation of each grid vertex's SDF before and after MLS correction}
	\label{fig:mls_std_dev}
\end{figure}
There are some examples of reconstructed surface shown in the Figure \ref{fig:mls_surf_iter_examples} and \ref{fig:mls_surf_iter_examples2} given different amount of iterations.
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSurfaceInitial.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSurface1Iteration.png}
			\caption{Mls correction 1 iteration}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSurface2Iteration.png}
			\caption{Mls correction 2 iterations}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSurface3Iteration.png}
			\caption{Mls correction 3 iterations}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSurface4Iteration.png}
			\caption{Mls correction 4 iterations}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/MlsSurface5Iteration.png}
			\caption{Mls correction 5 iterations}
		\end{subfigure}
	\end{center}
	\caption{Reconstructed MLS surface} \label{fig:mls_surf_iter_examples}
\end{figure}
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/MLS2SurfaceOriginal.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/Mls2Surface1Iteration.png}
			\caption{Mls correction 1 iteration}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/Mls2Surface2Iteration.png}
			\caption{Mls correction 2 iterations}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/Mls2Surface3Iteration.png}
			\caption{Mls correction 3 iterations}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/Mls2Surface4Iteration.png}
			\caption{Mls correction 4 iterations}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/Mls2Surface5Iteration.png}
			\caption{Mls correction 5 iterations}
		\end{subfigure}
	\end{center}
	\caption{Reconstructed MLS surface} 
	\label{fig:mls_surf_iter_examples2}
\end{figure}

The more iterations are applied for the MLS correction the smoother surface we receive. However, most of the smoothing work is done during the first two or three iterations. The Sum of squared errors falls very fast during the first two iterations and stays on the same level after following iterations (see figure \ref{fig:mls_std_dev}), which means that there is not much smoothing applied to the level set. Meantime, according to the figure \ref{fig:mls_surf_iter_examples} a large amount of iteration can destroy tiny fluid features, such as splashes and thin areas near the border of the fluid surface. This can be controlled via calibration of the smoothing factor, but for the purpose of this thesis, removal of small frequency bumps on the reconstructed surface, one iteration is enough in most cases.

\section{Performance analysis}
The performance of the MLS smoothing filter will be measured w.r.t. the number of MlsSamples, MaxMlsSamples, number of iterations. All measurements were performed on the AMD Ryzen 5 3550H. For transparent algorithm evaluation, all computations were performed on a single processor. All evaluations are displayed in seconds. Reconstruction was performed on the 2 frames of the fluid scene with 40000 SPH particles. Note that in the MLS reconstructions $updateLevelSet$ is a total of underlying methods initial $updateLevelSet$ and $mlsSmoothPath$ applied on top of that.\\
Table \ref{tab:mls_initial_method} shows the evaluated performance density based reconstruction method.
\begin{table}[H]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|c|}
			\hline
			Stage & time \\
			\hline
				configureHashTables	&	0.076903\\
				getTriangles	&	0.499455\\
				total execution time	&	6.029499\\
				updateGrid	&	1.337325\\
				updateLevelSet	&	3.736989\\
				updateSurfaceParticles	&	0.287444\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Initial performance evaluation for density based reconstruction}
	\label{tab:mls_initial_method}
\end{table}
Table \ref{tab:mls_ms_perf} shows evaluated performance of the MLS filter depending on the number of samples taken for the computation of MLS surface (MlsSamples). In this test MlsMaxSamples is explicitly set to the MlsSamples
\begin{table}[H]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|c|c|c|}
			\hline
			MlsSamples & 100 & 200 & 300 \\
			\hline
			configureHashTables		&	0.078537	&	0.076541	&	0.083138\\
			getTriangles			&	0.538212	&	0.510159	&	0.505589\\
			mlsSmoothPath			&	33.357061	&	72.295714	&	119.967774\\
			total execution time	&	39.713907	&	78.652220	&	126.201645\\
			updateGrid				&	1.459942	&	1.461316	&	1.437795\\
			updateLevelSet			&	37.277470	&	76.245051	&	123.824661\\
			updateSurfaceParticles	&	0.289628	&	0.287632	&	0.286882\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Initial performance evaluation for density based reconstruction}
	\label{tab:mls_ms_perf}
\end{table}
Table \ref{tab:mls_mms_perf} shows evaluated performance of the MLS filter depending on the number of maximum number of samples  (MaxMlsSamples) taking fixed number of MlsSamples, in this test MlsSamples=400.
\begin{table}[H]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|c|c|c|c|}
			\hline
			MaxMlsSamples & 50 & 100 & 150 & 200 \\
			\hline
			configureHashTables     	& 0.081193	&	0.081389	& 0.078795		& 0.077161\\
			getTriangles    			& 0.566251	&	0.565616	& 0.554958		& 0.535509\\
			mlsSmoothPath   			& 87.600709	&	94.290563	& 104.010487	& 116.572329\\
			total execution time    	& 94.378367	&	100.942730	& 110.744296	& 123.356513\\
			updateGrid      			& 1.585025	&	1.531106	& 1.586204		& 1.596259\\
			updateLevelSet  			& 91.765409	&	98.396836	& 108.144850	& 120.773621\\
			updateSurfaceParticles  	& 0.301292	&	0.298913	& 0.307338		& 0.308659\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Initial performance evaluation for density based reconstruction}
	\label{tab:mls_mms_perf}
\end{table}
Table \ref{tab:mls_iter_perf} shows evaluated performance of the MLS filter depending on the number of filter iterations taking fixed number of MlsSamples=200, and MaxSamples=50.
\begin{table}[H]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|c|c|c|c|}
			\hline
			Iterations & 1 & 2 & 3 & 4 \\
			\hline
			configureHashTables     	& 0.076567	&	0.084189	& 0.078997		& 0.083295\\
			getTriangles    			& 0.606721	&	0.583862	& 0.644775		& 0.524303\\
			mlsSmoothPath   			& 49.395048	&	97.056469	& 144.075857	& 187.602832\\
			total execution time    	& 56.223171	&	103.919529	& 151.014921	& 194.276611\\
			updateGrid      			& 1.578684	&	1.586247	& 1.566563		& 1.565801\\
			updateLevelSet  			& 53.585239	&	101.285508	& 148.343585	& 191.733618\\
			updateSurfaceParticles  	& 0.301436	&	0.311386	& 0.300734		& 0.304806\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Performance evaluation depending on the number of smoothing iterations}
	\label{tab:mls_iter_perf}
\end{table}

Table \ref{tab:cluster_factor} shows evaluated performance of the MLS filter depending on the ClusterFactor.
\begin{table}[H]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|c|c|c|c|c|}
			\hline
			ClusterFactor & 0.1 & 0.2 & 0.4 & 0.8 & 1 \\
			\hline
			configureHashTables    	&	0.076390	&	0.074464	&	0.076946	&	0.078014	&	0.077265\\
			getTriangles    		&	0.501665	&	0.503932	&	0.504590	&	0.510349	&	0.507719\\
			mlsSmoothPath   		&	4.791562	&	8.289250	&	15.761765	&	29.372052	&	36.577372\\
			total execution time   	&	13.610064	&	17.358378	&	24.563267	&	38.239182	&	45.699859\\
			updateGrid      		&	1.448008	&	1.456376	&	1.434868	&	1.465417	&	1.458499\\
			updateLevelSet  		&	11.196505	&	14.933524	&	22.164599	&	35.807620	&	43.258932\\
			updateSurfaceParticles 	&	0.291530	&	0.291450	&	0.290358	&	0.290106	&	0.293439\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Performance evaluation depending on the size of the ClusterFactor}
	\label{tab:cluster_factor}
\end{table}

In general, all parameters show linear time expansion on the reconstruction time for $mlsSmoothPath$ stage.

\section{Surface reconstruction results}
In this section some scenes representing the quality of the filter and reconstructed surface are shown. All simulations was generated using a SPlisHSPlasH framework \cite{SPlisHSPlasH}.\\
Figure \ref{fig:db_mls_reconstruction1} and \ref{fig:db_mls_reconstruction2} shows the reconstructed surface using density based Muller et.al. method. 
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMOriginal1.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMMls1.png}
			\caption{Mls correction smoothed}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMOriginal2.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMMls2.png}
			\caption{Mls correction smoothed}
		\end{subfigure}
	\end{center}
	\caption{Original density based surface with MLS smoothing comparison} \label{fig:db_mls_reconstruction1}
\end{figure}
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMOriginal3.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMMls3.png}
			\caption{Mls correction smoothed}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMOriginal4.png}
			\caption{original surface}
		\end{subfigure}\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/DDMMls4.png}
			\caption{Mls correction smoothed}
		\end{subfigure}
	\end{center}
	\caption{Original density based surface with MLS smoothed surface comparison} \label{fig:db_mls_reconstruction2}
\end{figure}
On Figures \ref{fig:db_mls_reconstruction3} and \ref{fig:db_mls_reconstruction4} surface reconstruction of canyon scene is presented.\\
Most of the high frequency bumps are smoothed. The final surfaces still preserve sharp features, however, some small details are smoothed out such as in figure \ref{fig:canion_small_features}. To achieve good smoothing results it is required to set up the enough mls samples in the neighborhood of each MC grid vertex, thus the computation time is significantly increased w.r.t. to the original reconstruction method e.g. the crown scene reconstructed with original Muller et.al. took ~10 seconds per frame (figure \ref{fig:crown_zb}), application of mls smoothing on top of the generated SDF brought additionally ~20 seconds to achieve smooth surface preserving tiny and sharp features (figure \ref{fig:crown_zb_mls}) 

\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionOriginal1.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionMls1.png}
			\caption{Mls correction smoothed}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionOriginal2.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionMls2.png}
			\caption{Mls correction smoothed}
			\label{fig:canion_small_features}
		\end{subfigure}
	\end{center}
	\caption{Reconstruction of the canyon scene with 2 million SPH particles.} \label{fig:db_mls_reconstruction3}
\end{figure}
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionOriginal3.png}
			\caption{Mls smoothed}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionMls3.png}
			\caption{original surface}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionOriginal4.png}
			\caption{Mls smoothed}
		\end{subfigure}
		\begin{subfigure}[b]{0.47\textwidth}
			\includegraphics[width=\textwidth]{figures/CanionMls4.png}
			\caption{original surface}
		\end{subfigure}
	\end{center}
	\caption{Reconstruction of the canyon scene with 13 million SPH particles } \label{fig:db_mls_reconstruction4}
\end{figure}

\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/MLSCrownZB.png}
			\caption{original surface}
			\label{fig:crown_zb}
		\end{subfigure}
		\begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/MLSCrownZBMLS.png}
			\caption{Mls smoothed}
			\label{fig:crown_zb_mls}
		\end{subfigure}
	\end{center}
	\caption{Surface reconstruction comparison for splash scene with 60000 SPH particles} \label{fig:crown_zb_mls}
\end{figure}
        
\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/DamBreackRaytracedNaiveMC.png}
			\caption{original surface}
			\label{fig:dam_break_zb}
		\end{subfigure}
		\begin{subfigure}[b]{\textwidth}
			\includegraphics[width=\textwidth]{figures/DamBreackRaytracedNaiveMCMls.png}
			\caption{Mls smoothed}
			\label{fig:dam_break_zb}
		\end{subfigure}
	\end{center}
	\caption{Dam break simulation with  40000 SPH particles} \label{fig:crown_zb_mls}
\end{figure}

        


\section{Conclusion and Future Work}
Mls smoothing filter is proven to be a good alternative to blur filter. The MLS filter is much more stable than the blur filter. As was already shown iterative application of the method converges to some defined surface. Iterative application of the MLS filter doesn't destroy continuous areas of the surface. The filter could be configured so, that small features, such that droplets or splashes in the reconstructed scene could be preserved, in the meantime undesired bumps are removed.\\
However, the most negative disadvantage of applying the MLS filter is its computation time. The total complexity of the algorithm is $O(n*m)$, where $n$ is a set of 0-level intersection cells and $m$ is a set of MlsSamples. By applying Monte Carlo approaches it is possible to uniformly sample $N \subseteq n$ and $M \subseteq m$, such that the total execution time of the reconstruction could be reduced, not sacrificing too much the final quality of the reconstructed surface.\\
The future work can be applied to massively parallelize the algorithm so that it can be run on the HPC clusters, or even port the computations on the GPU. In this work, OpenMP injections were applied to parallelize all stages of the reconstruction algorithms as well as reconstruction algorithms. However, the current implementation contains a lot of data dependencies, which limits the scalability of the algorithm on massively parallel systems like GPGPU's.\\ Another way the algorithm can be used is the generation of a dataset, which can be used as a training set for a deep multi-output regression neural network analogically to \cite{DNNNoiseFilter}.